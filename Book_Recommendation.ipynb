{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+EsUbBBbSHK4HKa820Rey"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"lF7pEGwEyTFh","executionInfo":{"status":"ok","timestamp":1734643214418,"user_tz":480,"elapsed":638,"user":{"displayName":"Anita","userId":"12238922703696688067"}}},"outputs":[],"source":["import pandas as pd\n","\n","# Example: Load dataset\n","books = pd.read_csv('books.csv')  # Book metadata\n","reviews = pd.read_csv('ratings.csv')  # User reviews"]},{"cell_type":"code","source":["print(books.columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wnu59l9TzAHB","executionInfo":{"status":"ok","timestamp":1734643290718,"user_tz":480,"elapsed":182,"user":{"displayName":"Anita","userId":"12238922703696688067"}},"outputId":"d9c61de3-b1f8-460c-aa0e-f5668ebbe356"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['book_id', 'goodreads_book_id', 'best_book_id', 'work_id',\n","       'books_count', 'isbn', 'isbn13', 'authors', 'original_publication_year',\n","       'original_title', 'title', 'language_code', 'average_rating',\n","       'ratings_count', 'work_ratings_count', 'work_text_reviews_count',\n","       'ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5',\n","       'image_url', 'small_image_url'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["books['combined_features'] = books['title'] + ' ' + books['authors']\n","books['processed_features'] = books['combined_features'].str.lower()"],"metadata":{"id":"G1TMs-TBzOJm","executionInfo":{"status":"ok","timestamp":1734643361799,"user_tz":480,"elapsed":155,"user":{"displayName":"Anita","userId":"12238922703696688067"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C1LtoAb20J0m","executionInfo":{"status":"ok","timestamp":1734643606083,"user_tz":480,"elapsed":183,"user":{"displayName":"Anita","userId":"12238922703696688067"}},"outputId":"5d94891e-d209-4319-c8e4-d536f3468b61"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string\n","\n","# Download NLTK stopwords\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","# Preprocessing function\n","def preprocess_text(text):\n","    text = text.lower()  # Convert to lowercase\n","    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n","    words = word_tokenize(text)  # Tokenize\n","    return ' '.join([word for word in words if word not in stop_words])  # Remove stopwords\n","\n","# Combine 'title' and 'authors' columns into a single feature\n","books['combined_features'] = books['title'] + ' ' + books['authors']\n","\n","# Apply the preprocessing function to the combined text\n","books['processed_features'] = books['combined_features'].apply(preprocess_text)\n","\n","# Optional: Check the result\n","print(books[['title', 'authors', 'processed_features']].head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":790},"id":"SoUm9SZayvdM","executionInfo":{"status":"error","timestamp":1734643609960,"user_tz":480,"elapsed":168,"user":{"displayName":"Anita","userId":"12238922703696688067"}},"outputId":"b4453b4a-12e5-4286-b809-f9fb775efcf6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"error","ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-51a6e3a0851b>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Apply the preprocessing function to the combined text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'combined_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Optional: Check the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-9-51a6e3a0851b>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# TF-IDF Vectorization\n","tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n","tfidf_matrix = tfidf.fit_transform(books['processed_features'])\n","\n","# Compute similarity\n","cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"],"metadata":{"id":"bsvj8cxW0aah","executionInfo":{"status":"ok","timestamp":1734643661515,"user_tz":480,"elapsed":2254,"user":{"displayName":"Anita","userId":"12238922703696688067"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def get_recommendations(title, cosine_sim=cosine_sim):\n","    indices = pd.Series(books.index, index=books['title']).drop_duplicates()\n","    if title not in indices:\n","        return \"Book not found in the dataset.\"\n","\n","    idx = indices[title]\n","    sim_scores = list(enumerate(cosine_sim[idx]))\n","    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","    sim_scores = sim_scores[1:11]  # Top 10 recommendations\n","\n","    book_indices = [i[0] for i in sim_scores]\n","    return books.iloc[book_indices][['title', 'authors']]"],"metadata":{"id":"Dnp01au00cIb","executionInfo":{"status":"ok","timestamp":1734643674588,"user_tz":480,"elapsed":196,"user":{"displayName":"Anita","userId":"12238922703696688067"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["recommendations = get_recommendations('Harry Potter and the Sorcerer\\'s Stone')\n","print(recommendations)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nyrVOGCX0fwm","executionInfo":{"status":"ok","timestamp":1734643685815,"user_tz":480,"elapsed":139,"user":{"displayName":"Anita","userId":"12238922703696688067"}},"outputId":"d3bed4de-9e69-41d8-ddfc-b6e793dc3428"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Book not found in the dataset.\n"]}]},{"cell_type":"code","source":["!pip install scikit-surprise"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atcjvdFWArMX","executionInfo":{"status":"ok","timestamp":1734646954777,"user_tz":480,"elapsed":82349,"user":{"displayName":"Anita","userId":"12238922703696688067"}},"outputId":"ee535956-6cd8-447e-951a-ee05a28dd4b5"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-surprise\n","  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.13.1)\n","Building wheels for collected packages: scikit-surprise\n","  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357278 sha256=3799c3973a4087a5a1d8d2c7d09473567f5280c408be9d2a85d9d0567d0b07da\n","  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n","Successfully built scikit-surprise\n","Installing collected packages: scikit-surprise\n","Successfully installed scikit-surprise-1.1.4\n"]}]},{"cell_type":"code","source":["from surprise import Dataset, Reader, SVD\n","from surprise.model_selection import cross_validate\n","\n","# Prepare the ratings data\n","ratings_data = books[['goodreads_book_id', 'average_rating']]\n","\n","# Add a 'user_id' column. Here, we just use a dummy user_id for simplicity\n","ratings_data['user_id'] = 1  # or you could create a series of unique user IDs if needed\n","\n","# Rename columns for surprise compatibility\n","ratings_data = ratings_data.rename(columns={'goodreads_book_id': 'item_id', 'average_rating': 'rating'})\n","\n","# Set up the Reader\n","reader = Reader(rating_scale=(1, 5))\n","\n","# Load the dataset using surprise's load_from_df\n","data = Dataset.load_from_df(ratings_data[['user_id', 'item_id', 'rating']], reader)\n","\n","# Train a model (SVD as an example)\n","model = SVD()\n","model.fit(data.build_full_trainset())\n","\n","# Cross-validation to evaluate the model\n","cross_validate(model, data, cv=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ADdmFIuAEckK","executionInfo":{"status":"ok","timestamp":1734648372860,"user_tz":480,"elapsed":1380,"user":{"displayName":"Anita","userId":"12238922703696688067"}},"outputId":"9ba82b57-6060-42bc-b7d3-80970265171c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-25-a1b003e707c3>:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  ratings_data['user_id'] = 1  # or you could create a series of unique user IDs if needed\n"]},{"output_type":"execute_result","data":{"text/plain":["{'test_rmse': array([0.25385435, 0.25365695, 0.25732319]),\n"," 'test_mae': array([0.19963105, 0.19633613, 0.2025452 ]),\n"," 'fit_time': (0.1221320629119873, 0.12379312515258789, 0.12244749069213867),\n"," 'test_time': (0.016458511352539062,\n","  0.014517784118652344,\n","  0.014871597290039062)}"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# Function to get top N book recommendations for a user\n","def get_top_n_recommendations(user_id, n=10):\n","    # Generate a list of all books (item_ids)\n","    all_books = books['goodreads_book_id'].unique()\n","\n","    # List to store predictions\n","    predictions = []\n","\n","    # Predict ratings for all books\n","    for book_id in all_books:\n","        pred = model.predict(user_id, book_id)\n","        predictions.append((book_id, pred.est))\n","\n","    # Sort predictions by predicted rating (highest first)\n","    predictions.sort(key=lambda x: x[1], reverse=True)\n","\n","    # Get the top N book recommendations\n","    top_n_books = predictions[:n]\n","\n","    # Return the top N books (book_id and predicted rating)\n","    return top_n_books\n","\n","# Get top 10 book recommendations for the user\n","user_id = 1  # Replace with a unique user ID if you have one\n","top_books = get_top_n_recommendations(user_id, n=10)"],"metadata":{"id":"aNvryLRxFK0D","executionInfo":{"status":"ok","timestamp":1734648388905,"user_tz":480,"elapsed":308,"user":{"displayName":"Anita","userId":"12238922703696688067"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Convert top N book recommendations to titles\n","top_books_df = books[books['goodreads_book_id'].isin([book[0] for book in top_books])]\n","top_books_df['predicted_rating'] = [book[1] for book in top_books]\n","\n","# Display the top N books with their predicted ratings\n","print(top_books_df[['title', 'predicted_rating']])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-d4iRekFMGB","executionInfo":{"status":"ok","timestamp":1734648402107,"user_tz":480,"elapsed":151,"user":{"displayName":"Anita","userId":"12238922703696688067"}},"outputId":"3965d6f8-2df0-48ee-f37c-86183ce76672"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  title  predicted_rating\n","26    Harry Potter and the Half-Blood Prince (Harry ...          4.077297\n","3240                 Crooked Kingdom (Six of Crows, #2)          4.074241\n","3274  Harry Potter Boxed Set, Books 1-5 (Harry Potte...          4.072156\n","3735  Harry Potter Page to Screen: The Complete Film...          4.070909\n","4482  It's a Magical World: A Calvin and Hobbes Coll...          4.070443\n","4777           The Holy Bible: English Standard Version          4.066517\n","5918                  Life Application Study Bible: NIV          4.061294\n","8853                           Mark of the Lion Trilogy          4.061122\n","9359         The Green Mile, Part 6: Coffey on the Mile          4.061098\n","9565  Attack of the Deranged Mutant Killer Monster S...          4.061064\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-27-71908bade061>:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  top_books_df['predicted_rating'] = [book[1] for book in top_books]\n"]}]},{"cell_type":"code","source":["# Get the user input for the author name\n","author_name = input(\"Enter the author you're interested in: \").lower()\n","\n","# Filter books that match the author\n","books_by_author = books[books['authors'].str.lower().str.contains(author_name, na=False)].copy()\n","\n","# Calculate weighted rating for filtered books\n","books_by_author.loc[:, 'weighted_rating'] = books_by_author['average_rating'] * books_by_author['ratings_count']\n","\n","# Sort the filtered books by average rating in descending order (highest rating first)\n","recommended_books_by_author_weighted = books_by_author.sort_values(by='average_rating', ascending=False)\n","\n","# Display the top recommended books by the selected author with average rating\n","print(f\"\\nTop recommended books by {author_name.title()} (based on average ratings):\")\n","print(recommended_books_by_author_weighted[['title', 'average_rating', 'ratings_count', 'weighted_rating']].head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B57cGgrTKvO3","executionInfo":{"status":"ok","timestamp":1734649954315,"user_tz":480,"elapsed":3482,"user":{"displayName":"Anita","userId":"12238922703696688067"}},"outputId":"4f1c794d-3453-43b2-a2a4-48318bfcd82e"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the author you're interested in: Stephen King\n","\n","Top recommended books by Stephen King (based on average ratings):\n","                                                  title  average_rating  \\\n","9359         The Green Mile, Part 6: Coffey on the Mile            4.55   \n","2092                          The Stand: Soul Survivors            4.52   \n","9138                Carrie / 'Salem's Lot / The Shining            4.52   \n","8975  Rita Hayworth and Shawshank Redemption: A Stor...            4.52   \n","8370  The Green Mile, Part 4: The Bad Death of Eduar...            4.52   \n","\n","      ratings_count  weighted_rating  \n","9359          11936         54308.80  \n","2092          40626        183629.52  \n","9138          11063         50004.76  \n","8975          11499         51975.48  \n","8370          12958         58570.16  \n"]}]},{"cell_type":"code","source":["# Display the top recommended books by the selected author, excluding the 'authors' column\n","recommended_books_by_author = books_by_top_author.sort_values(by='average_rating', ascending=False)\n","print(f\"\\nTop recommended books by {top_author}:\")\n","print(recommended_books_by_author[['title', 'average_rating', 'ratings_count']].head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FD45ALgXGxvs","executionInfo":{"status":"ok","timestamp":1734650172118,"user_tz":480,"elapsed":150,"user":{"displayName":"Anita","userId":"12238922703696688067"}},"outputId":"25587bf6-94e7-4011-ff6c-07258053dbf9"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Top recommended books by Stephen King:\n","                                                  title  average_rating  \\\n","9359         The Green Mile, Part 6: Coffey on the Mile            4.55   \n","2092                          The Stand: Soul Survivors            4.52   \n","9138                Carrie / 'Salem's Lot / The Shining            4.52   \n","8975  Rita Hayworth and Shawshank Redemption: A Stor...            4.52   \n","8370  The Green Mile, Part 4: The Bad Death of Eduar...            4.52   \n","\n","      ratings_count  \n","9359          11936  \n","2092          40626  \n","9138          11063  \n","8975          11499  \n","8370          12958  \n"]}]}]}